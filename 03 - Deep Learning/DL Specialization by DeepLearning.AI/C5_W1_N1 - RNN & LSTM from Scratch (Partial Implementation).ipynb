{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef77cf6-da02-490c-9e70-fcca472d89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6e1a5-9084-40fc-9115-99aa8e64c849",
   "metadata": {},
   "source": [
    "# Heplper Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d074287-38e3-4cb8-bfcb-7488188aab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x -- Shape of (n_classes, batch_size)\n",
    "    \"\"\"\n",
    "        \n",
    "    x_shifted = x - np.max(x, axis=0, keepdims=True)\n",
    "    e_x = np.exp(x_shifted)\n",
    "    probs = e_x / e_x.sum(axis=0, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a12d80-8889-4b6a-b16b-fbb1e623d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_clipped = np.clip(x, -500, 500)\n",
    "    s = 1 / (1 + np.exp(-x_clipped))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470606cf-05cf-4c02-93c0-8f575a05f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of sequences to the same length.\n",
    "\n",
    "    Arguments:\n",
    "    sequences -- list of numpy arrays of shape (embedding_size, seq_len_i)\n",
    "    max_len -- int, maximum length to pad to. If None, use the longest sequence in the batch.\n",
    "    padding_value -- value to pad with, default 0 (usually for PAD token)\n",
    "\n",
    "    Returns:\n",
    "    padded_seqs -- numpy array of shape (embedding_size, batch_size, max_len)\n",
    "    mask -- numpy array of shape (max_len, batch_size), 1 for valid tokens, 0 for PAD\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(sequences)\n",
    "    embedding_size = sequences[0].shape[0]\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = max(seq.shape[1] for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((embedding_size, batch_size, max_len), padding_value)\n",
    "    mask = np.zeros((max_len, batch_size))\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = seq.shape[1]\n",
    "        padded_seqs[:, i, :seq_len] = seq\n",
    "        mask[:seq_len, i] = 1\n",
    "\n",
    "    return padded_seqs, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567aabc9-c3b8-409a-b7dc-567ef6712aee",
   "metadata": {},
   "source": [
    "# 1. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ac9cd-08eb-4021-a76b-186442e2c2c8",
   "metadata": {},
   "source": [
    "### 1.1. RNN Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249ddae-1830-4ede-aa39-f94c88a54470",
   "metadata": {},
   "source": [
    "You can think of the recurrent neural network as the repeated use of a single cell. First, you'll implement the computations for a single time step. The following figure describes the operations for a single time step of an RNN cell: \n",
    "\n",
    "<img src=\"images/rnnCell_forward.png\" style=\"width:100%;height:auto;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $\\hat{y}^{\\langle t \\rangle}$ \n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f952a01-4e63-4124-ac8a-f45c52efa8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnCell_forward(xt, a_prev, parameters, mask_t=None):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN cell\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "                    Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                    Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                    Ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                    Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                    By -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    mask -- (optional) Binary mask of shape (1, m), 1 for valid tokens, 0 for PAD\n",
    "    \n",
    "    Returns:\n",
    "    at -- next hidden state at timestep \"t\", numpy array of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (at, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    Ba = parameters[\"Ba\"]\n",
    "    By = parameters[\"By\"]\n",
    "    \n",
    "    at = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + Ba)  # output_shape(n_a, m)\n",
    "\n",
    "    if mask_t is not None:\n",
    "        at = mask_t * at + (1 - mask_t) * a_prev  # keeps the previous hidden state in PAD position\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wya, at) + By)  # output_shape(n_y, m)\n",
    "    \n",
    "    # assuming n_y is vocab_size (n_y = n_x = vocab_size) to simplify this homework\n",
    "    # the output at each timestamp must be in the shape of (vocab_size, batch_size) to predict the next token, you can do another layer of linear projection with W(vocab_size, n_y)\n",
    "    \n",
    "    cache = (at, a_prev, xt, parameters, mask_t)\n",
    "    return at, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33270361-8bd1-4cd5-8fbf-3ee9950e6399",
   "metadata": {},
   "source": [
    "- A recurrent neural network (RNN) is a repetition of the RNN cell that you've just built. \n",
    "    - If your input sequence of data is 10 time steps long, then you will re-use the RNN cell 10 times \n",
    "- Each cell takes two inputs at each time step:\n",
    "    - $a^{\\langle t-1 \\rangle}$: The hidden state from the previous cell\n",
    "    - $x^{\\langle t \\rangle}$: The current time step's input data\n",
    "- It has two outputs at each time step:\n",
    "    - A hidden state ($a^{\\langle t \\rangle}$)\n",
    "    - A prediction ($y^{\\langle t \\rangle}$)\n",
    "- The weights and biases $(W_{aa}, W_{ax}, b_{a}, W_{ay}, b_{y})$ are re-used each time step \n",
    "    - They are maintained between calls to `rnnCell_forward` in the 'parameters' dictionary\n",
    "\n",
    "<img src=\"images/rnnSeq_forward.png\" style=\"width:100%;height:auto;\">\n",
    "<caption><center><font color='purple'><b>Figure 3</b>: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle Tx \\rangle})$  is carried over $Tx$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle Tx \\rangle})$. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90428f5-7bc0-4560-95d0-a9527dc90b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters, mask=None):\n",
    "    \"\"\"\n",
    "    Implements forward propagation through every timestep for RNNs\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every timestep (padded), numpy array of shape (n_x, m, Tx).\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing: Waa, Wax, Wya, Ba, By\n",
    "    mask -- (Optional) Binary mask of shape (Tx, m), 1 for valid tokens, 0 for PAD\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every timestep, numpy array of shape (n_a, m, Tx)\n",
    "    y_pred -- Predictions for every timestep, numpy array of shape (n_y, m, Tx)\n",
    "    caches -- a tuple of caches for every timestep needed for the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x, m, Tx = x.shape  # Tx is the length of all inputs in batch\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "\n",
    "    # Storage\n",
    "    a = np.zeros((n_a, m, Tx))\n",
    "    y_pred = np.zeros((n_y, m, Tx))\n",
    "    caches = []\n",
    "\n",
    "    # Temp Storage\n",
    "    a_prev = a0\n",
    "\n",
    "    # Loop over timestep\n",
    "    for t in range(Tx):\n",
    "        # Get mask at timestep t if exists\n",
    "        mask_t = None\n",
    "        if mask is not None:\n",
    "            mask_t = mask[t].reshape(1, -1) # shape(1, m)\n",
    "        \n",
    "        # Forward through cell\n",
    "        at, yt_pred, cache = rnnCell_forward(x[:,:,t], a_prev, parameters, mask_t)\n",
    "        \n",
    "        # Store\n",
    "        a[:,:,t] = at\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # Reset hidden state\n",
    "        a_prev = at\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5eec61-7032-48ef-b3ad-e0d55c02552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y_true, mask=None):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss for a sequence of predictions, with optional mask.\n",
    "\n",
    "    Arguments:\n",
    "    y_pred -- numpy array of shape (n_y, m, Tx), predicted probabilities (after softmax)\n",
    "    y_true -- numpy array of shape (n_y, m, Tx), one-hot true labels\n",
    "    mask -- numpy array of shape (Tx, m), 1 for valid tokens, 0 for PAD\n",
    "\n",
    "    Returns:\n",
    "    loss -- scalar, average cross-entropy loss over valid tokens\n",
    "    \"\"\"\n",
    "\n",
    "    n_y, m, Tx = y_pred.shape\n",
    "    \n",
    "    epsilon = 1e-8 # avoid log(0)\n",
    "    y_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "\n",
    "    ce_loss = (-y_true) * np.log(y_pred_clipped) # output_shape(n_y, m, Tx)\n",
    "    ce_loss = np.sum(ce_loss, axis=0) # output_shape(m, Tx) => loss for sample i at timestep t\n",
    "\n",
    "    if mask is not None:\n",
    "        ce_loss = ce_loss * mask.T\n",
    "        total_tokens = np.sum(mask)\n",
    "    else:\n",
    "        total_tokens = m * Tx\n",
    "\n",
    "    # Loss averaged by batch and timestep\n",
    "    avg_loss = np.sum(ce_loss) / total_tokens\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38fffe-1a1c-4da7-88c4-4c42a92af454",
   "metadata": {},
   "source": [
    "### 1.2. RNN Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de7cf3-432e-4db0-946f-45e4957590c2",
   "metadata": {},
   "source": [
    "Begin by computing the backward pass for the basic RNN cell. Then, in the following sections, iterate through the cells.\n",
    "\n",
    "<img src=\"images/rnnCell_Backward_equations.png\" alt=\"RNN Cell Backward Pass\" style=\"width:100%; height:auto;\">\n",
    "<br>\n",
    "<caption><center><font color='purple'><b>Figure 6</b>: The RNN cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ Backpropagates through the time steps of the RNN By following the chain rule from calculus. Internal to the cell, the chain rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. The operation can utilize the cached results from the forward path. </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf9106-36c1-454c-a2d9-3dd2b1d18133",
   "metadata": {},
   "source": [
    "<img src=\"images/rnnCell_Backward.png\" style=\"width:800px;height:500px;\"> <br>\n",
    "<caption>\n",
    "    <center>\n",
    "        <font color='purple'><b>Figure 7</b>: This implementation of <code>rnnCell_backward</code> doesn't include the output dense layer and softmax.  \n",
    "    </center>\n",
    "</caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b74e27-6d1d-4660-b6c5-823c0fb44f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnCell_backward(da_next, dz_t, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN cell (single time step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to the next hidden state at timestep \"t\", of shape (n_a, m)\n",
    "    dz_t -- Gradient of loss with respect to the prediction before softmax at timestep \"t\", of shape (n_y, m)\n",
    "    cache -- Tuple of values needed for the backward pass, contains (at, a_prev, xt, parameters, mask_t)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Python dictionary containing:\n",
    "                    dxt -- Gradients of input data, of shape (n_x, m)\n",
    "                    da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                    dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                    dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                    dBa -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                    dWya  -- Gradient w.r.t hidden-to-output weights Wya, shape (n_y, n_a)\n",
    "                    dBy   -- Gradient w.r.t output bias By, shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    (a_next, a_prev, xt, parameters, mask_t) = cache\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    Ba = parameters[\"Ba\"]\n",
    "    By = parameters[\"By\"]\n",
    "\n",
    "    # Mask here to avoid updating Wy, By at padding positions \n",
    "    if mask_t is not None:\n",
    "        dz_t = dz_t * mask_t # (n_y, m) @ (1, m)\n",
    "        \n",
    "    # Gradients from the output at each timestep\n",
    "    # You can compute those gradients in \"rnn_backward\" using vectorization, but I put it here for clarity\n",
    "    dWya = np.dot(dz_t, a_next.T) # (n_y, m) @ (n_a, m).T\n",
    "    dBy = np.sum(dz_t, axis=1, keepdims=True) # (n_y, 1)\n",
    "    da_next_from_z = np.dot(Wya.T, dz_t) # (n_y, n_a).T @ (n_y, m)\n",
    "\n",
    "    # Total gradient for a_next\n",
    "    da_total = da_next + da_next_from_z # (n_a, m)\n",
    "    \n",
    "    # Gradient through the TANH function that computes a_next\n",
    "    dtanh = da_total * (1 - a_next**2)  # (n_a, m)\n",
    "\n",
    "    # Mask here to avoid updating Wa, Ba at padding positions\n",
    "    if mask_t is not None:\n",
    "        dtanh = dtanh * mask_t  # (n_a, m) @ (1, m)\n",
    "\n",
    "    # Gradients for parameters that compute a_next\n",
    "    dWax = np.dot(dtanh, xt.T)  # (n_a, m) @ (n_x, m).T\n",
    "    dWaa = np.dot(dtanh, a_prev.T)  # (n_a, m) @ (n_a, m).T\n",
    "    dBa = np.sum(dtanh, axis=1, keepdims=True)  # (n_a, 1)\n",
    "\n",
    "    # Gradients for input\n",
    "    dxt = np.dot(Wax.T, dtanh)  # (n_a, n_x).T  @ (n_a, m)\n",
    "    da_prev = np.dot(Waa.T, dtanh)  # (n_a, n_a).T @ (n_a, m)\n",
    "\n",
    "    # Mask here to avoid affecting the previous time-step\n",
    "    if mask_t is not None:\n",
    "        da_prev = da_prev * mask_t\n",
    "        dxt = dxt * mask_t\n",
    "    \n",
    "    gradients_t = {\n",
    "        \"dxt\": dxt, \n",
    "        \"da_prev\": da_prev, \n",
    "        \"dWax\": dWax, \n",
    "        \"dWaa\": dWaa,\n",
    "        \"dBa\": dBa, \n",
    "        \"dWya\": dWya, \n",
    "        \"dBy\": dBy\n",
    "    }\n",
    "    return gradients_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a87d6b-145a-4014-a214-ab69e8c307a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(y_pred, y_true, caches):\n",
    "    (a_next, a_prev, xt, parameters, _) = caches[0]\n",
    "    (n_y, m, Tx) = y_pred.shape\n",
    "    n_x = xt.shape[0]\n",
    "    n_a = a_next.shape[0]\n",
    "\n",
    "    # Storage\n",
    "    dx = np.zeros((n_x, m, Tx))\n",
    "    dWax = np.zeros_like(parameters[\"Wax\"])\n",
    "    dWaa = np.zeros_like(parameters[\"Waa\"])\n",
    "    dBa = np.zeros_like(parameters[\"Ba\"])\n",
    "    dWya = np.zeros_like(parameters[\"Wya\"])\n",
    "    dBy = np.zeros_like(parameters[\"By\"])\n",
    "\n",
    "    # Temp Storage\n",
    "    da_t = np.zeros((n_a, m)) # the last timestep da is zero\n",
    "\n",
    "    dz = y_pred - y_true # (n_y, m, Tx)\n",
    "    # da_from_output = np.tensordot(Wya.T, dz, axes=([1], [0]))  # (n_y, n_a).T @ (n_y, m, Tx) = (n_a, m, Tx)\n",
    "    # ignored because it's already handled in each cell, although that way is not optimal because we can utilize vectorization\n",
    "    \n",
    "    # Backward\n",
    "    for t in reversed(range(Tx)):\n",
    "        cache = caches[t]\n",
    "        gradients_t = rnnCell_backward(da_t, dz[:,:,t], cache)\n",
    "        \n",
    "        # Store\n",
    "        dx[:,:,t] = gradients_t[\"dxt\"]\n",
    "        dWax += gradients_t[\"dWax\"]\n",
    "        dWaa += gradients_t[\"dWaa\"]\n",
    "        dBa += gradients_t[\"dBa\"]\n",
    "        dWya += gradients_t[\"dWya\"]\n",
    "        dBy += gradients_t[\"dBy\"]\n",
    "        \n",
    "        # Reset the gradient of the hidden state to the previous one\n",
    "        da_t = gradients_t[\"da_prev\"]\n",
    "\n",
    "    da0 = da_t\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    # Sometimes people also normalize by Tx to avoid very large gradients for long sequences, but it’s optional.\n",
    "    dWax /= m\n",
    "    dWaa /= m\n",
    "    dBa /= m\n",
    "    dWya /= m\n",
    "    dBy /= m\n",
    "    \n",
    "    gradients = {\n",
    "        \"dx\": dx,\n",
    "        \"da0\": da0,\n",
    "        \"dWax\": dWax,\n",
    "        \"dWaa\": dWaa,\n",
    "        \"dBa\": dBa,\n",
    "        \"dWya\": dWya,\n",
    "        \"dBy\": dBy\n",
    "    }\n",
    "\n",
    "    # Storing dx if we have another embedding layer needed to train\n",
    "    # Storing da0 if we use stateful RNN\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c88cde-b6bb-40e0-b9e1-e2e9fdb4a081",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b6cb9-278f-45e2-b419-89a2b6f468c7",
   "metadata": {},
   "source": [
    "### 2.1. LSTM Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ad3af-e0aa-4c98-ac5f-d7e6f0310b87",
   "metadata": {},
   "source": [
    "This following figure shows the operations of an LSTM cell.\n",
    "\n",
    "<img src=\"images/lstmCell_forward.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> <b>Figure 4</b>: LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>\n",
    "\n",
    "Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. \n",
    "\n",
    "### About the gates\n",
    "\n",
    "#### - Forget gate\n",
    "\n",
    "For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: \n",
    "\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "\n",
    "Here, $W_f$ are weights that govern the forget gate's behavior. We concatenate $[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$ and multiply by $W_f$. The equation above results in a vector $\\Gamma_f^{\\langle t \\rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\\langle t-1 \\rangle}$. So if one of the values of $\\Gamma_f^{\\langle t \\rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\\langle t-1 \\rangle}$. If one of the values is 1, then it will keep the information. \n",
    "\n",
    "#### - Update gate\n",
    "\n",
    "Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: \n",
    "\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
    "\n",
    "Similar to the forget gate, here $\\Gamma_u^{\\langle t \\rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\\tilde{c}^{\\langle t \\rangle}$, in order to compute $c^{\\langle t \\rangle}$.\n",
    "\n",
    "#### - Updating the cell \n",
    "\n",
    "To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: \n",
    "\n",
    "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "\n",
    "Finally, the new cell state is: \n",
    "\n",
    "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "\n",
    "#### - Output gate\n",
    "\n",
    "To decide which outputs we will use, we will use the following two formulas: \n",
    "\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\\tanh$ of the previous state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87a265d-52b6-494b-9169-c39ff3b668df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmCell_forward(xt, a_prev, c_prev, parameters, mask_t=None):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "                    Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                    Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                    Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                    bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                    Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                    Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                    by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters, gates)\n",
    "    \"\"\"\n",
    "    \n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    input_concated = np.vstack((a_prev, xt))  # output_shape(n_a + n_x, m)\n",
    "\n",
    "    # Compute new cell state\n",
    "    ft = sigmoid(np.dot(Wf, input_concated) + bf)  # output_shape(n_a, m)\n",
    "    it = sigmoid(np.dot(Wi, input_concated) + bi)  # output_shape(n_a, m)\n",
    "    cct = np.tanh(np.dot(Wc, input_concated) + bc) # output_shape(n_a, m)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "\n",
    "    # Compute new hidden state\n",
    "    ot = sigmoid(np.dot(Wo, input_concated) + bo) # output_shape(n_a, m)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "\n",
    "    # Compute output at timestep \"t\"\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by) # output_shape(n_y, m)\n",
    "\n",
    "    # Apply mask\n",
    "    if mask_t is not None:\n",
    "        a_next = mask_t * a_next + (1 - mask_t) * a_prev\n",
    "        c_next = mask_t * c_next + (1 - mask_t) * c_prev\n",
    "\n",
    "    # Cache\n",
    "    gates = (ft, it, cct, ot)\n",
    "    cache = (a_next, c_next, a_prev, c_prev, xt, parameters, gates, mask_t)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ac8da-436c-46a8-8a47-2499cc0864ce",
   "metadata": {},
   "source": [
    "Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. \n",
    "\n",
    "<img src=\"images/lstmSeq_forward.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> <b>Figure 3</b>: LSTM over multiple time-steps. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c2decd-c014-4d75-815e-c2fd5a3efb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, c0, parameters, mask=None):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, Tx).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    c0 -- Initial cell state, of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, Tx)\n",
    "    c -- Cell states for every time-step, numpy array of shape (n_a, m, Tx)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, Tx)\n",
    "    caches -- tuple of caches for every time-step needed for the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x, m, Tx = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # Storage\n",
    "    a = np.zeros((n_a, m, Tx))\n",
    "    c = np.zeros((n_a, m, Tx))\n",
    "    y_pred = np.zeros((n_y, m, Tx))\n",
    "    caches = []\n",
    "    \n",
    "    # Temp Storage\n",
    "    a_prev = a0\n",
    "    c_prev = c0\n",
    "\n",
    "    # Forward\n",
    "    for t in range(Tx):\n",
    "        # Get mask at timestep t if exists\n",
    "        mask_t = None\n",
    "        if mask is not None:\n",
    "            mask_t = mask[t].reshape(1, -1)  # shape (1, m)\n",
    "\n",
    "        # Forward\n",
    "        a_next, c_next, yt_pred, cache = lstmCell_forward(x[:,:,t], a_prev, c_prev, parameters, mask_t)\n",
    "        \n",
    "        # Store\n",
    "        a[:,:,t] = a_next\n",
    "        c[:,:,t]  = c_next\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        caches.append(cache)\n",
    "\n",
    "        # Reset hidden & cell states\n",
    "        a_prev = a_next\n",
    "        c_prev = c_next\n",
    "\n",
    "    return a, c, y_pred, caches   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf89c4-39b5-42a0-85f0-8605994299dd",
   "metadata": {},
   "source": [
    "### 2.2 LSTM Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7e90f-89c8-45da-8bbc-958a5a16d35d",
   "metadata": {},
   "source": [
    "### Gate derivatives\n",
    "\n",
    "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
    "\n",
    "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
    "\n",
    "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
    "\n",
    "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
    "\n",
    "### Parameter derivatives \n",
    "\n",
    "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
    "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
    "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
    "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
    "\n",
    "To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$ respectively. Note that you should have the `keep_dims = True` option.\n",
    "\n",
    "### Derivatives with respect to the previous hidden state, previous memory state, and input.\n",
    "\n",
    "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
    "Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...)\n",
    "\n",
    "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
    "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
    "where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f647c339-2cd4-4deb-9b0b-6863d0d1903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmCell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients_t -- Python dictionary containing:\n",
    "                    dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "                    da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                    dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "                    dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                    dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                    dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                    dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                    dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # gates = (ft, it, cct, ot)\n",
    "    # cache = (a_next, c_next, a_prev, c_prev, xt, parameters, gates, mask_t)\n",
    "    (a_next, c_next, a_prev, c_prev, xt, parameters, gates, mask_t) = cache\n",
    "    (ft, it, cct, ot) = gates\n",
    "    \n",
    "    n_x, m = xt.shape\n",
    "    n_a, _ = a_next.shape\n",
    "\n",
    "    # Gradients of Loss to Gates\n",
    "    dot = da_next * np.tanh(c_next) # (n_a, m)\n",
    "    dct = dc_next + da_next * ot * (1 - np.tanh(c_next)**2)\n",
    "    dft = dct * c_prev                     \n",
    "    dit = dct * cct                        \n",
    "    dcct = dct * it\n",
    "    \n",
    "    dft_raw = dft * ft * (1 - ft) # (n_a, m)\n",
    "    dit_raw = dit * it * (1 - it)\n",
    "    dcct_raw = dcct * (1 - cct**2)\n",
    "    dot_raw = dot * ot * (1 - ot)\n",
    "\n",
    "    # Apply mask to avoid updating parameters in PAD positions\n",
    "    if mask_t is not None:\n",
    "        dft_raw = dft_raw * mask_t\n",
    "        dit_raw = dit_raw * mask_t\n",
    "        dcct_raw = dcct_raw * mask_t\n",
    "        dot_raw = dot_raw * mask_t\n",
    "\n",
    "    # Gradients of Parameters\n",
    "    input_concated = np.vstack((a_prev, xt))  # (n_a + n_x, m)\n",
    "    \n",
    "    dWf = np.dot(dft_raw, input_concated.T) # (n_a, m) @ (n_ax, m).T = (n_a, n_ax)\n",
    "    dWi = np.dot(dit_raw, input_concated.T)\n",
    "    dWc = np.dot(dcct_raw, input_concated.T)\n",
    "    dWo = np.dot(dot_raw, input_concated.T)\n",
    "\n",
    "    dbf = np.sum(dft_raw, axis=1, keepdims=True) # (n_a, 1)\n",
    "    dbi = np.sum(dit_raw, axis=1, keepdims=True)\n",
    "    dbc = np.sum(dcct_raw, axis=1, keepdims=True)\n",
    "    dbo = np.sum(dot_raw, axis=1, keepdims=True)\n",
    "\n",
    "    # Gradients of input, hidden state, cell state\n",
    "    dz = (np.dot(Wf.T, dft_raw) + np.dot(Wi.T, dit_raw) + np.dot(Wc.T, dcct_raw) + np.dot(Wo.T, dot_raw))  # (n_ax, m)\n",
    "    da_prev = dz[:n_a, :] # (n_a, m)\n",
    "    dxt = dz[n_a:, :]     # (n_x, m)\n",
    "    dc_prev = dct * ft    # (n_a, m)\n",
    "\n",
    "    # Apply mask\n",
    "    if mask_t is not None:\n",
    "        da_prev = da_prev * mask_t\n",
    "        dc_prev = dc_prev * mask_t\n",
    "        dxt = dxt * mask_t\n",
    "\n",
    "    # Returns\n",
    "    gradients_t = {\n",
    "        \"dxt\": dxt,\n",
    "        \"da_prev\": da_prev,\n",
    "        \"dc_prev\": dc_prev,\n",
    "        \"dWf\": dWf,\n",
    "        \"dWi\": dWi,\n",
    "        \"dWc\": dWc,\n",
    "        \"dWo\": dWo,\n",
    "        \"dbf\": dbf,\n",
    "        \"dbi\": dbi,\n",
    "        \"dbc\": dbc,\n",
    "        \"dbo\": dbo\n",
    "    }\n",
    "\n",
    "    return gradients_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce4ef09-1063-4bb9-9211-2f2fb26b0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(y_pred, y_true, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM over all timesteps.\n",
    "\n",
    "    Arguments:\n",
    "    y_pred -- Predictions for all timesteps, shape (n_y, m, Tx)\n",
    "    y_true -- True labels, shape (n_y, m, Tx)\n",
    "    caches -- List of caches from lstm_forward\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Python dictionary containing:\n",
    "                    dx -- Gradient of inputs, shape (n_x, m, Tx)\n",
    "                    da0 -- Gradient of initial hidden state, shape (n_a, m)\n",
    "                    dc0 -- Gradient of initial cell state, shape (n_a, m)\n",
    "                    dWf, dWi, dWc, dWo -- Gradients of LSTM weights\n",
    "                    dbf, dbi, dbc, dbo -- Gradients of LSTM biases\n",
    "                    dWy, dby -- Gradients of output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    (a_next, c_next, a_prev, c_prev, xt, parameters, _, _) = caches[0]\n",
    "    n_y, m, Tx = y_pred.shape\n",
    "    n_x = xt.shape[0]\n",
    "    n_a = a_next.shape[0]\n",
    "\n",
    "    # Storage\n",
    "    dx = np.zeros((n_x, m, Tx))\n",
    "    dWf = np.zeros_like(parameters[\"Wf\"])\n",
    "    dWi = np.zeros_like(parameters[\"Wi\"])\n",
    "    dWc = np.zeros_like(parameters[\"Wc\"])\n",
    "    dWo = np.zeros_like(parameters[\"Wo\"])\n",
    "    dbf = np.zeros_like(parameters[\"bf\"])\n",
    "    dbi = np.zeros_like(parameters[\"bi\"])\n",
    "    dbc = np.zeros_like(parameters[\"bc\"])\n",
    "    dbo = np.zeros_like(parameters[\"bo\"])\n",
    "    dWy = np.zeros_like(parameters[\"Wy\"])\n",
    "    dby = np.zeros_like(parameters[\"by\"])\n",
    "\n",
    "    # Temp Storage\n",
    "    da_t = np.zeros((n_a, m))\n",
    "    dc_t = np.zeros((n_a, m))\n",
    "\n",
    "    # Gradient of Loss to output before softmax\n",
    "    dz = y_pred - y_true  # (n_y, m, Tx)    \n",
    "    \n",
    "    for t in reversed(range(Tx)):\n",
    "        cache = caches[t]\n",
    "        a_next, c_next, a_prev, c_prev, xt, parameters, _, mask_t = cache\n",
    "        \n",
    "        dz_t = dz[:,:,t]  # (n_y, m)\n",
    "        if mask_t is not None:\n",
    "            dz_t = dz_t * mask_t\n",
    "\n",
    "        # Gradient from output\n",
    "        dWy += np.dot(dz_t, a_next.T)  # (n_y, m) @ (n_a, m).T\n",
    "        dby += np.sum(dz_t, axis=1, keepdims=True)  # (n_y, 1)\n",
    "        da_from_z = np.dot(parameters[\"Wy\"].T, dz_t)  # (n_y, n_a).T @ (n_y, m)\n",
    "\n",
    "        # Total da_t from the next timestep & output\n",
    "        da_total = da_t + da_from_z\n",
    "\n",
    "        # Backward LSTM cell\n",
    "        gradients_t = lstmCell_backward(da_total, dc_t, cache)\n",
    "\n",
    "        # Store\n",
    "        dx[:, :, t] = gradients_t[\"dxt\"]\n",
    "        dWf += gradients_t[\"dWf\"]\n",
    "        dWi += gradients_t[\"dWi\"]\n",
    "        dWc += gradients_t[\"dWc\"]\n",
    "        dWo += gradients_t[\"dWo\"]\n",
    "        dbf += gradients_t[\"dbf\"]\n",
    "        dbi += gradients_t[\"dbi\"]\n",
    "        dbc += gradients_t[\"dbc\"]\n",
    "        dbo += gradients_t[\"dbo\"]\n",
    "\n",
    "        # Reset current hidden/cell gradients\n",
    "        da_t = gradients_t[\"da_prev\"]\n",
    "        dc_t = gradients_t[\"dc_prev\"]\n",
    "\n",
    "    da0 = da_t\n",
    "    dc0 = dc_t\n",
    "\n",
    "    gradients = {\n",
    "        \"dx\": dx,\n",
    "        \"da0\": da0,\n",
    "        \"dc0\": dc0,\n",
    "        \"dWf\": dWf / m,\n",
    "        \"dWi\": dWi / m,\n",
    "        \"dWc\": dWc / m,\n",
    "        \"dWo\": dWo / m,\n",
    "        \"dbf\": dbf / m,\n",
    "        \"dbi\": dbi / m,\n",
    "        \"dbc\": dbc / m,\n",
    "        \"dbo\": dbo / m,\n",
    "        \"dWy\": dWy / m,\n",
    "        \"dby\": dby / m\n",
    "    }\n",
    " \n",
    "    return gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
